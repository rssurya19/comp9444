{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-28T18:34:15.868098Z",
     "iopub.status.busy": "2025-07-28T18:34:15.867935Z",
     "iopub.status.idle": "2025-07-28T18:34:16.347929Z",
     "shell.execute_reply": "2025-07-28T18:34:16.347711Z",
     "shell.execute_reply.started": "2025-07-28T18:34:15.868085Z"
    },
    "id": "euWEtL6ECQ4f",
    "outputId": "df553471-3886-4a02-9b21-47b354215374"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 'coco-caption/pycocoevalcap' to Python path.\n",
      "NLTK data downloaded successfully\n"
     ]
    }
   ],
   "source": [
    "# # Cell 0 (Add this before Cell 1, or run it in your terminal)\n",
    "# !pip install evaluate\n",
    "# !pip install transformers # Ensure transformers is also up-to-date\n",
    "# !pip install accelerate # Often a dependency for transformers, good to have\n",
    "# !pip install datasets # Often useful for data handling in HF ecosystem\n",
    "# !pip install torch torchvision torchaudio\n",
    "# !pip install nltk\n",
    "# !pip install scikit-learn\n",
    "# !pip install pycocoevalcap\n",
    "\n",
    "# For COCO-related metrics (CIDEr, SPICE):\n",
    "# !pip install pycocotools\n",
    "# !git clone https://github.com/tylin/coco-caption.git\n",
    "\n",
    "# Add coco-caption/pycocoevalcap to your Python path\n",
    "import sys\n",
    "if 'coco-caption/pycocoevalcap' not in sys.path:\n",
    "    sys.path.append('coco-caption/pycocoevalcap')\n",
    "    print(\"Added 'coco-caption/pycocoevalcap' to Python path.\")\n",
    "\n",
    "# You may also need to install nltk data for some tokenizers used by pycocoevalcap\n",
    "import nltk\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('punkt_tab', quiet=True)\n",
    "    print(\"NLTK data downloaded successfully\")\n",
    "except:\n",
    "    print(\"NLTK download failed, but continuing...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-28T18:34:16.348477Z",
     "iopub.status.busy": "2025-07-28T18:34:16.348309Z",
     "iopub.status.idle": "2025-07-28T18:34:19.240219Z",
     "shell.execute_reply": "2025-07-28T18:34:19.240007Z",
     "shell.execute_reply.started": "2025-07-28T18:34:16.348466Z"
    },
    "id": "XVK_nTCMAHYv",
    "outputId": "fc6ce4c0-8a91-4b82-b39a-d62df7df6a18"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 04:34:18.162317: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-29 04:34:18.169584: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753727658.178183  296502 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753727658.181002  296502 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1753727658.188074  296502 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753727658.188083  296502 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753727658.188084  296502 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753727658.188085  296502 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-29 04:34:18.190347: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO evaluation tools imported successfully\n",
      "Using device: cuda\n",
      "PyTorch version: 2.8.0.dev20250410+cu128\n"
     ]
    }
   ],
   "source": [
    "# Import all necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import (\n",
    "    ViTImageProcessor,\n",
    "    ViTModel,\n",
    "    AutoTokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    get_scheduler\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "import evaluate\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For evaluation metrics (CIDEr and SPICE)\n",
    "try:\n",
    "    from pycocotools.coco import COCO\n",
    "    from pycocoevalcap.eval import COCOEvalCap\n",
    "    print(\"COCO evaluation tools imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"Warning: COCO evaluation tools not available. Install pycocotools and coco-caption\")\n",
    "\n",
    "# Set device and random seeds for reproducibility\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# # Define image directory and caption file\n",
    "# IMAGE_DIR = '/content/image'\n",
    "# CAPTION_FILE = '/content/new_dataset_rscid.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-28T18:34:19.240672Z",
     "iopub.status.busy": "2025-07-28T18:34:19.240467Z",
     "iopub.status.idle": "2025-07-28T18:34:19.243526Z",
     "shell.execute_reply": "2025-07-28T18:34:19.243189Z",
     "shell.execute_reply.started": "2025-07-28T18:34:19.240664Z"
    },
    "id": "n-iepQkhAvtV",
    "outputId": "07bcc43d-131a-4dc6-f14d-dd3621bfb80b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "  IMAGE_DIR: /home/shay/Desktop/UNSW/9444/project/39/RSICD_optimal-master/RSICD_optimal-master/RSICD_images/RSICD_images\n",
      "  CAPTION_FILE: dataset_rsicd.json\n",
      "  BATCH_SIZE: 16\n",
      "  LEARNING_RATE: 0.0001\n",
      "  NUM_EPOCHS: 20\n",
      "  MAX_CAPTION_LENGTH: 100\n",
      "  MAX_GENERATION_LENGTH: 50\n",
      "  PATIENCE: 5\n",
      "  VIT_MODEL: google/vit-base-patch16-224\n",
      "  GPT2_MODEL: gpt2\n",
      "  MODEL_SAVE_PATH: best_image_captioning_model.pth\n"
     ]
    }
   ],
   "source": [
    "# CELL 2: Configuration and Paths\n",
    "# ============================================================================\n",
    "\n",
    "# Configuration parameters\n",
    "CONFIG = {\n",
    "    'IMAGE_DIR': '/home/shay/Desktop/UNSW/9444/project/39/RSICD_optimal-master/RSICD_optimal-master/RSICD_images/RSICD_images',  # Update this path\n",
    "    'CAPTION_FILE': 'dataset_rsicd.json',  # Update this path\n",
    "    'BATCH_SIZE': 16,\n",
    "    'LEARNING_RATE': 1e-4,\n",
    "    'NUM_EPOCHS': 20,\n",
    "    'MAX_CAPTION_LENGTH': 100,\n",
    "    'MAX_GENERATION_LENGTH': 50,\n",
    "    'PATIENCE': 5,  # For early stopping\n",
    "    'VIT_MODEL': \"google/vit-base-patch16-224\",\n",
    "    'GPT2_MODEL': \"gpt2\",\n",
    "    'MODEL_SAVE_PATH': \"best_image_captioning_model.pth\"\n",
    "}\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "os.makedirs('plots', exist_ok=True)\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T18:34:19.244362Z",
     "iopub.status.busy": "2025-07-28T18:34:19.244236Z",
     "iopub.status.idle": "2025-07-28T18:34:19.754245Z",
     "shell.execute_reply": "2025-07-28T18:34:19.753987Z",
     "shell.execute_reply.started": "2025-07-28T18:34:19.244352Z"
    },
    "id": "9U512fygAyep"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CREATING DATASET SPLITS AND JSON FILES\n",
      "============================================================\n",
      "Loading dataset from: dataset_rsicd.json\n",
      "Total images in dataset: 10921\n",
      "✓ Saved train split:\n",
      "  - File: ./rsicd_train.json\n",
      "  - Images: 8734\n",
      "  - Annotations: 43670\n",
      "✓ Saved val split:\n",
      "  - File: ./rsicd_val.json\n",
      "  - Images: 1094\n",
      "  - Annotations: 5470\n",
      "✓ Saved test split:\n",
      "  - File: ./rsicd_test.json\n",
      "  - Images: 1093\n",
      "  - Annotations: 5465\n",
      "\n",
      "Total split distribution:\n",
      "  train: 8734 images\n",
      "  val: 1094 images\n",
      "  test: 1093 images\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: Dataset Splitting and JSON File Creation\n",
    "# ============================================================================\n",
    "\n",
    "def create_split_datasets(caption_file, output_dir='./'):\n",
    "    \"\"\"\n",
    "    Create train/val/test splits from the original dataset and save as separate JSON files\n",
    "    This implements the \"create json file when split\" requirement\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"CREATING DATASET SPLITS AND JSON FILES\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Load the original dataset JSON\n",
    "    print(f\"Loading dataset from: {caption_file}\")\n",
    "    with open(caption_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    print(f\"Total images in dataset: {len(data['images'])}\")\n",
    "\n",
    "    # Initialize split dictionaries in COCO format\n",
    "    split_data = {\n",
    "        \"train\": {\n",
    "            \"images\": [],\n",
    "            \"annotations\": [],\n",
    "            \"info\": {\"description\": \"RSICD Training Set\"},\n",
    "            \"licenses\": []\n",
    "        },\n",
    "        \"val\": {\n",
    "            \"images\": [],\n",
    "            \"annotations\": [],\n",
    "            \"info\": {\"description\": \"RSICD Validation Set\"},\n",
    "            \"licenses\": []\n",
    "        },\n",
    "        \"test\": {\n",
    "            \"images\": [],\n",
    "            \"annotations\": [],\n",
    "            \"info\": {\"description\": \"RSICD Test Set\"},\n",
    "            \"licenses\": []\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Counters\n",
    "    split_counts = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
    "    annotation_id_counter = 0\n",
    "\n",
    "    # Process each image in the dataset\n",
    "    for item in data['images']:\n",
    "        split = item['split']\n",
    "        split_counts[split] += 1\n",
    "\n",
    "        # Create image entry for COCO format\n",
    "        image_entry = {\n",
    "            \"id\": len(split_data[split][\"images\"]),\n",
    "            \"width\": 256,  # Default size for RSICD\n",
    "            \"height\": 256,\n",
    "            \"file_name\": item['filename'],\n",
    "            \"original_id\": item['imgid']\n",
    "        }\n",
    "        split_data[split][\"images\"].append(image_entry)\n",
    "\n",
    "        # Create annotation entries for each caption\n",
    "        for sentence in item['sentences']:\n",
    "            annotation_entry = {\n",
    "                \"id\": annotation_id_counter,\n",
    "                \"image_id\": image_entry['id'],\n",
    "                \"caption\": sentence['raw'].strip(),\n",
    "                \"tokens\": sentence['tokens'] if 'tokens' in sentence else sentence['raw'].split()\n",
    "            }\n",
    "            split_data[split][\"annotations\"].append(annotation_entry)\n",
    "            annotation_id_counter += 1\n",
    "\n",
    "    # Save each split to a separate JSON file\n",
    "    saved_files = {}\n",
    "    for split_name, split_content in split_data.items():\n",
    "        output_filename = os.path.join(output_dir, f\"rsicd_{split_name}.json\")\n",
    "        with open(output_filename, 'w') as f:\n",
    "            json.dump(split_content, f, indent=2)\n",
    "        saved_files[split_name] = output_filename\n",
    "\n",
    "        print(f\"✓ Saved {split_name} split:\")\n",
    "        print(f\"  - File: {output_filename}\")\n",
    "        print(f\"  - Images: {len(split_content['images'])}\")\n",
    "        print(f\"  - Annotations: {len(split_content['annotations'])}\")\n",
    "\n",
    "    print(f\"\\nTotal split distribution:\")\n",
    "    for split, count in split_counts.items():\n",
    "        print(f\"  {split}: {count} images\")\n",
    "\n",
    "    return saved_files, split_data\n",
    "\n",
    "# Create the dataset splits\n",
    "split_files, split_data = create_split_datasets(CONFIG['CAPTION_FILE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-07-28T18:34:19.754626Z",
     "iopub.status.busy": "2025-07-28T18:34:19.754545Z",
     "iopub.status.idle": "2025-07-28T18:34:19.759203Z",
     "shell.execute_reply": "2025-07-28T18:34:19.759016Z",
     "shell.execute_reply.started": "2025-07-28T18:34:19.754620Z"
    },
    "id": "Pq1-7a0zA6us",
    "outputId": "5857277b-1038-4a41-c499-dedabb953924"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 4: Dataset Class Implementation\n",
    "# ============================================================================\n",
    "\n",
    "class RSICDDataset(Dataset):\n",
    "    \"\"\"\n",
    "    RSICD Dataset class for loading images and captions\n",
    "    \"\"\"\n",
    "    def __init__(self, json_file, img_dir, processor, tokenizer, split_type=\"train\", max_length=100):\n",
    "        print(f\"Initializing {split_type} dataset from {json_file}\")\n",
    "\n",
    "        self.img_dir = img_dir\n",
    "        self.processor = processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.split_type = split_type\n",
    "\n",
    "        # Set pad token\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        # Load data from JSON file\n",
    "        with open(json_file, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "\n",
    "        # Create mappings\n",
    "        self.images = self.data['images']\n",
    "        self.annotations = self.data['annotations']\n",
    "\n",
    "        # Create image_id to captions mapping\n",
    "        self.image_id_to_captions = defaultdict(list)\n",
    "        for ann in self.annotations:\n",
    "            self.image_id_to_captions[ann['image_id']].append(ann['caption'])\n",
    "\n",
    "        # Create image_id to filename mapping\n",
    "        self.image_id_to_filename = {img['id']: img['file_name'] for img in self.images}\n",
    "\n",
    "        # Create samples list for iteration (image_filename, caption pairs)\n",
    "        self.samples = []\n",
    "        for ann in self.annotations:\n",
    "            img_filename = self.image_id_to_filename[ann['image_id']]\n",
    "            self.samples.append((img_filename, ann['caption'], ann['image_id']))\n",
    "\n",
    "        print(f\"✓ Loaded {len(self.samples)} samples from {len(self.images)} unique images\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_filename, caption, image_id = self.samples[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_filename)\n",
    "\n",
    "        # Load and process image\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error loading image {img_path}: {e}\")\n",
    "            # Create a black placeholder image\n",
    "            image = Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "\n",
    "        # Process image for ViT\n",
    "        pixel_values = self.processor(images=image, return_tensors=\"pt\").pixel_values.squeeze(0)\n",
    "\n",
    "        # Tokenize caption for GPT-2\n",
    "        caption_encoding = self.tokenizer(\n",
    "            caption,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        caption_tokens = caption_encoding.input_ids.squeeze(0)\n",
    "\n",
    "        return {\n",
    "            'pixel_values': pixel_values,\n",
    "            'caption_tokens': caption_tokens,\n",
    "            'caption_text': caption,\n",
    "            'image_filename': img_filename,\n",
    "            'image_id': image_id\n",
    "        }\n",
    "\n",
    "    def get_image_captions(self, image_id):\n",
    "        \"\"\"Get all captions for a specific image\"\"\"\n",
    "        return self.image_id_to_captions[image_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "2a78bd1e63fd40328b9fc7849f968ae0",
      "d6c27fe6c52b402c8b3b7a4df3736559",
      "0ea4402503be43c4af682772898dfea7",
      "956003538f464073a066b70b9440dd82",
      "8681c344d6db448c8e4d193b6428a1aa",
      "b61b39a4f17f46c7861d337a85a04a9e",
      "cfc381380edf4b4ea2bc5fcc017841a4",
      "18ad727136da4b31814855ce1eca35b5",
      "3bad1a902ba249b9854979d85a0af0f6",
      "818cb93aaf2c4fad922d08b87e2a46cd",
      "333243f6bf76493fbf3df8dd3082b3da"
     ]
    },
    "execution": {
     "iopub.execute_input": "2025-07-28T18:34:19.759674Z",
     "iopub.status.busy": "2025-07-28T18:34:19.759540Z",
     "iopub.status.idle": "2025-07-28T18:34:19.765557Z",
     "shell.execute_reply": "2025-07-28T18:34:19.765334Z",
     "shell.execute_reply.started": "2025-07-28T18:34:19.759663Z"
    },
    "id": "tBRST2DSA8Q_",
    "outputId": "4e0d8fbb-de77-491a-ce02-2f85c9d6311d"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: ViT-GPT2 Model Implementation\n",
    "# ============================================================================\n",
    "\n",
    "class ViTGPT2Model(nn.Module):\n",
    "    \"\"\"\n",
    "    ViT + GPT-2 Model for Image Captioning\n",
    "    This implements the \"model (ViT + GPT2)\" requirement\n",
    "    \"\"\"\n",
    "    def __init__(self, vit_model_name=\"google/vit-base-patch16-224\", gpt2_model_name=\"gpt2\"):\n",
    "        super(ViTGPT2Model, self).__init__()\n",
    "\n",
    "        print(\"Initializing ViT-GPT2 Model...\")\n",
    "\n",
    "        # Load pre-trained models\n",
    "        self.vit = ViTModel.from_pretrained(vit_model_name)\n",
    "        self.gpt2 = GPT2LMHeadModel.from_pretrained(gpt2_model_name)\n",
    "        self.processor = ViTImageProcessor.from_pretrained(vit_model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(gpt2_model_name)\n",
    "\n",
    "        # Set pad token for GPT-2\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        if self.gpt2.config.pad_token_id is None:\n",
    "            self.gpt2.config.pad_token_id = self.gpt2.config.eos_token_id\n",
    "\n",
    "        # Projection layer to map ViT features to GPT-2 hidden dimension\n",
    "        self.image_projection = nn.Linear(\n",
    "            self.vit.config.hidden_size,\n",
    "            self.gpt2.config.hidden_size\n",
    "        )\n",
    "\n",
    "        # Use pooling to reduce ViT sequence length (fixes position embedding issue)\n",
    "        self.feature_pooling = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        print(f\"✓ ViT hidden size: {self.vit.config.hidden_size}\")\n",
    "        print(f\"✓ GPT-2 hidden size: {self.gpt2.config.hidden_size}\")\n",
    "        print(f\"✓ GPT-2 max position embeddings: {self.gpt2.config.max_position_embeddings}\")\n",
    "\n",
    "    def forward(self, pixel_values, caption_tokens=None):\n",
    "        batch_size = pixel_values.shape[0]\n",
    "        device = pixel_values.device\n",
    "\n",
    "        # Extract image features using ViT\n",
    "        vit_outputs = self.vit(pixel_values=pixel_values)\n",
    "        image_features = vit_outputs.last_hidden_state  # (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        # Pool image features to reduce sequence length\n",
    "        # This prevents position embedding overflow\n",
    "        pooled_features = self.feature_pooling(image_features.transpose(1, 2)).transpose(1, 2)\n",
    "        # pooled_features shape: (batch_size, 1, hidden_size)\n",
    "\n",
    "        # Project to GPT-2 hidden dimension\n",
    "        image_embeds = self.image_projection(pooled_features)\n",
    "        image_embeds = self.dropout(image_embeds)\n",
    "\n",
    "        if caption_tokens is not None:\n",
    "            # Training mode\n",
    "            # Get text embeddings\n",
    "            text_embeds = self.gpt2.transformer.wte(caption_tokens)\n",
    "\n",
    "            # Concatenate image and text embeddings\n",
    "            combined_embeds = torch.cat([image_embeds, text_embeds], dim=1)\n",
    "\n",
    "            # Create attention mask\n",
    "            batch_size, seq_len = caption_tokens.shape\n",
    "            image_mask = torch.ones(batch_size, 1, dtype=torch.long, device=device)\n",
    "            text_mask = (caption_tokens != self.tokenizer.pad_token_id).long()\n",
    "            attention_mask = torch.cat([image_mask, text_mask], dim=1)\n",
    "\n",
    "            # Create labels for language modeling\n",
    "            image_labels = torch.full((batch_size, 1), -100, dtype=torch.long, device=device)\n",
    "            combined_labels = torch.cat([image_labels, caption_tokens], dim=1)\n",
    "\n",
    "            # Forward pass through GPT-2\n",
    "            outputs = self.gpt2(\n",
    "                inputs_embeds=combined_embeds,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=combined_labels\n",
    "            )\n",
    "\n",
    "            return outputs\n",
    "        else:\n",
    "            # Inference mode - return image embeddings\n",
    "            return image_embeds\n",
    "\n",
    "    def generate_caption(self, pixel_values, max_length=50, num_beams=1, do_sample=False, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Generate captions for given images\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        device = pixel_values.device\n",
    "        batch_size = pixel_values.shape[0]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Get image embeddings\n",
    "            image_embeds = self.forward(pixel_values)\n",
    "\n",
    "            generated_captions = []\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                current_image_embed = image_embeds[i:i+1]  # (1, 1, hidden_size)\n",
    "\n",
    "                # Initialize with start token\n",
    "                input_ids = torch.tensor([[self.tokenizer.eos_token_id]], device=device)\n",
    "                generated_tokens = []\n",
    "\n",
    "                for step in range(max_length):\n",
    "                    # Get current text embeddings\n",
    "                    current_text_embeds = self.gpt2.transformer.wte(input_ids)\n",
    "\n",
    "                    # Combine image and text embeddings\n",
    "                    combined_embeds = torch.cat([current_image_embed, current_text_embeds], dim=1)\n",
    "\n",
    "                    # Create attention mask\n",
    "                    attention_mask = torch.ones(1, combined_embeds.shape[1], device=device)\n",
    "\n",
    "                    # Forward pass\n",
    "                    outputs = self.gpt2(\n",
    "                        inputs_embeds=combined_embeds,\n",
    "                        attention_mask=attention_mask\n",
    "                    )\n",
    "\n",
    "                    # Get next token logits\n",
    "                    next_token_logits = outputs.logits[0, -1, :]\n",
    "\n",
    "                    # Apply temperature and sample\n",
    "                    if do_sample and temperature > 0:\n",
    "                        next_token_logits = next_token_logits / temperature\n",
    "                        probs = F.softmax(next_token_logits, dim=-1)\n",
    "                        next_token = torch.multinomial(probs, num_samples=1)\n",
    "                    else:\n",
    "                        next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "\n",
    "                    # Stop if EOS token is generated\n",
    "                    if next_token.item() == self.tokenizer.eos_token_id:\n",
    "                        break\n",
    "\n",
    "                    generated_tokens.append(next_token.item())\n",
    "                    input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1)\n",
    "\n",
    "                # Decode generated tokens\n",
    "                if generated_tokens:\n",
    "                    caption = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "                else:\n",
    "                    caption = \"\"\n",
    "\n",
    "                generated_captions.append(caption)\n",
    "\n",
    "        return generated_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T18:34:19.765829Z",
     "iopub.status.busy": "2025-07-28T18:34:19.765763Z",
     "iopub.status.idle": "2025-07-28T18:34:20.775487Z",
     "shell.execute_reply": "2025-07-28T18:34:20.775065Z",
     "shell.execute_reply.started": "2025-07-28T18:34:19.765823Z"
    },
    "id": "Fd7Jh3mxA8_y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "INITIALIZING DATASETS AND DATALOADERS\n",
      "============================================================\n",
      "Initializing train dataset from rsicd_train.json\n",
      "✓ Loaded 43670 samples from 8734 unique images\n",
      "Initializing val dataset from rsicd_val.json\n",
      "✓ Loaded 5470 samples from 1094 unique images\n",
      "Initializing test dataset from rsicd_test.json\n",
      "✓ Loaded 5465 samples from 1093 unique images\n",
      "✓ Training samples: 43670\n",
      "✓ Validation samples: 5470\n",
      "✓ Test samples: 5465\n",
      "✓ Training batches: 2730\n",
      "✓ Validation batches: 342\n",
      "✓ Test batches: 342\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 6: Initialize Datasets and DataLoaders\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INITIALIZING DATASETS AND DATALOADERS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize processors and tokenizers\n",
    "image_processor = ViTImageProcessor.from_pretrained(CONFIG['VIT_MODEL'])\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG['GPT2_MODEL'])\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = RSICDDataset(\n",
    "    json_file=\"rsicd_train.json\",\n",
    "    img_dir=CONFIG['IMAGE_DIR'],\n",
    "    processor=image_processor,\n",
    "    tokenizer=tokenizer,\n",
    "    split_type=\"train\",\n",
    "    max_length=CONFIG['MAX_CAPTION_LENGTH']\n",
    ")\n",
    "\n",
    "val_dataset = RSICDDataset(\n",
    "    json_file=\"rsicd_val.json\",\n",
    "    img_dir=CONFIG['IMAGE_DIR'],\n",
    "    processor=image_processor,\n",
    "    tokenizer=tokenizer,\n",
    "    split_type=\"val\",\n",
    "    max_length=CONFIG['MAX_CAPTION_LENGTH']\n",
    ")\n",
    "\n",
    "test_dataset = RSICDDataset(\n",
    "    json_file=\"rsicd_test.json\",\n",
    "    img_dir=CONFIG['IMAGE_DIR'],\n",
    "    processor=image_processor,\n",
    "    tokenizer=tokenizer,\n",
    "    split_type=\"test\",\n",
    "    max_length=CONFIG['MAX_CAPTION_LENGTH']\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function for batching\"\"\"\n",
    "    pixel_values = torch.stack([item['pixel_values'] for item in batch])\n",
    "    caption_tokens = torch.stack([item['caption_tokens'] for item in batch])\n",
    "    caption_texts = [item['caption_text'] for item in batch]\n",
    "    image_filenames = [item['image_filename'] for item in batch]\n",
    "    image_ids = [item['image_id'] for item in batch]\n",
    "\n",
    "    return {\n",
    "        'pixel_values': pixel_values,\n",
    "        'caption_tokens': caption_tokens,\n",
    "        'caption_texts': caption_texts,\n",
    "        'image_filenames': image_filenames,\n",
    "        'image_ids': image_ids\n",
    "    }\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG['BATCH_SIZE'],\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG['BATCH_SIZE'],\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=CONFIG['BATCH_SIZE'],\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "print(f\"✓ Training samples: {len(train_dataset)}\")\n",
    "print(f\"✓ Validation samples: {len(val_dataset)}\")\n",
    "print(f\"✓ Test samples: {len(test_dataset)}\")\n",
    "print(f\"✓ Training batches: {len(train_dataloader)}\")\n",
    "print(f\"✓ Validation batches: {len(val_dataloader)}\")\n",
    "print(f\"✓ Test batches: {len(test_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T18:34:20.775851Z",
     "iopub.status.busy": "2025-07-28T18:34:20.775769Z",
     "iopub.status.idle": "2025-07-28T18:34:24.742147Z",
     "shell.execute_reply": "2025-07-28T18:34:24.741854Z",
     "shell.execute_reply.started": "2025-07-28T18:34:20.775843Z"
    },
    "id": "bhx3_4uJBCG9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing evaluation metrics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/shay/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/shay/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/shay/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Hugging Face metrics loaded\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 7: Evaluation Metrics Implementation\n",
    "# ============================================================================\n",
    "\n",
    "class EvaluationMetrics:\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation metrics for image captioning\n",
    "    This implements: BLEU 1-4, METEOR, CIDEr, ROUGE_L, SPICE\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        print(\"Initializing evaluation metrics...\")\n",
    "\n",
    "        # Initialize Hugging Face metrics\n",
    "        try:\n",
    "            self.bleu_metric = evaluate.load(\"bleu\")\n",
    "            self.meteor_metric = evaluate.load(\"meteor\")\n",
    "            self.rouge_metric = evaluate.load(\"rouge\")\n",
    "            print(\"✓ Hugging Face metrics loaded\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load some Hugging Face metrics: {e}\")\n",
    "            self.bleu_metric = None\n",
    "            self.meteor_metric = None\n",
    "            self.rouge_metric = None\n",
    "\n",
    "    def compute_bleu_scores(self, predictions, references):\n",
    "        \"\"\"Compute BLEU 1-4 scores\"\"\"\n",
    "        bleu_scores = {'bleu_1': 0, 'bleu_2': 0, 'bleu_3': 0, 'bleu_4': 0}\n",
    "\n",
    "        if self.bleu_metric is None:\n",
    "            return bleu_scores\n",
    "\n",
    "        try:\n",
    "            # BLEU-1\n",
    "            bleu_1 = self.bleu_metric.compute(\n",
    "                predictions=predictions,\n",
    "                references=references,\n",
    "                max_order=1\n",
    "            )['bleu']\n",
    "\n",
    "            # BLEU-2\n",
    "            bleu_2 = self.bleu_metric.compute(\n",
    "                predictions=predictions,\n",
    "                references=references,\n",
    "                max_order=2\n",
    "            )['bleu']\n",
    "\n",
    "            # BLEU-3\n",
    "            bleu_3 = self.bleu_metric.compute(\n",
    "                predictions=predictions,\n",
    "                references=references,\n",
    "                max_order=3\n",
    "            )['bleu']\n",
    "\n",
    "            # BLEU-4\n",
    "            bleu_4 = self.bleu_metric.compute(\n",
    "                predictions=predictions,\n",
    "                references=references,\n",
    "                max_order=4\n",
    "            )['bleu']\n",
    "\n",
    "            bleu_scores = {\n",
    "                'bleu_1': bleu_1,\n",
    "                'bleu_2': bleu_2,\n",
    "                'bleu_3': bleu_3,\n",
    "                'bleu_4': bleu_4\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error computing BLEU scores: {e}\")\n",
    "\n",
    "        return bleu_scores\n",
    "\n",
    "    def compute_meteor_score(self, predictions, references):\n",
    "        \"\"\"Compute METEOR score\"\"\"\n",
    "        if self.meteor_metric is None:\n",
    "            return 0.0\n",
    "\n",
    "        try:\n",
    "            meteor_score = self.meteor_metric.compute(\n",
    "                predictions=predictions,\n",
    "                references=references\n",
    "            )['meteor']\n",
    "            return meteor_score\n",
    "        except Exception as e:\n",
    "            print(f\"Error computing METEOR score: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    def compute_rouge_l_score(self, predictions, references):\n",
    "        \"\"\"Compute ROUGE-L score\"\"\"\n",
    "        if self.rouge_metric is None:\n",
    "            return 0.0\n",
    "\n",
    "        try:\n",
    "            rouge_score = self.rouge_metric.compute(\n",
    "                predictions=predictions,\n",
    "                references=references,\n",
    "                rouge_types=['rougeL']\n",
    "            )['rougeL']\n",
    "            return rouge_score\n",
    "        except Exception as e:\n",
    "            print(f\"Error computing ROUGE-L score: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    def compute_coco_metrics(self, predictions_dict, references_dict):\n",
    "        \"\"\"\n",
    "        Compute CIDEr and SPICE using COCO evaluation tools\n",
    "        predictions_dict: {image_id: generated_caption}\n",
    "        references_dict: {image_id: [reference_captions]}\n",
    "        \"\"\"\n",
    "        metrics = {'cider': 0.0, 'spice': 0.0}\n",
    "\n",
    "        try:\n",
    "            # Prepare data in COCO format\n",
    "            # Ground truth annotations\n",
    "            ground_truth = {\n",
    "                \"annotations\": [],\n",
    "                \"images\": [],\n",
    "                \"info\": {\"description\": \"Ground truth captions\"},\n",
    "                \"licenses\": []\n",
    "            }\n",
    "\n",
    "            # Results (predictions)\n",
    "            results = []\n",
    "\n",
    "            ann_id = 0\n",
    "            for image_id, ref_captions in references_dict.items():\n",
    "                # Add image info\n",
    "                ground_truth[\"images\"].append({\n",
    "                    \"id\": image_id,\n",
    "                    \"width\": 256,\n",
    "                    \"height\": 256,\n",
    "                    \"file_name\": f\"image_{image_id}.jpg\"\n",
    "                })\n",
    "\n",
    "                # Add reference annotations\n",
    "                for ref_caption in ref_captions:\n",
    "                    ground_truth[\"annotations\"].append({\n",
    "                        \"id\": ann_id,\n",
    "                        \"image_id\": image_id,\n",
    "                        \"caption\": ref_caption.strip()\n",
    "                    })\n",
    "                    ann_id += 1\n",
    "\n",
    "                # Add prediction\n",
    "                if image_id in predictions_dict:\n",
    "                    results.append({\n",
    "                        \"image_id\": image_id,\n",
    "                        \"caption\": predictions_dict[image_id].strip()\n",
    "                    })\n",
    "\n",
    "            # Save temporary files\n",
    "            gt_file = \"temp_gt.json\"\n",
    "            res_file = \"temp_res.json\"\n",
    "\n",
    "            with open(gt_file, 'w') as f:\n",
    "                json.dump(ground_truth, f)\n",
    "\n",
    "            with open(res_file, 'w') as f:\n",
    "                json.dump(results, f)\n",
    "\n",
    "            # Evaluate using COCO tools\n",
    "            coco = COCO(gt_file)\n",
    "            coco_result = coco.loadRes(res_file)\n",
    "            coco_eval = COCOEvalCap(coco, coco_result)\n",
    "            coco_eval.params['image_id'] = coco_result.getImgIds()\n",
    "            coco_eval.evaluate()\n",
    "\n",
    "            # Extract metrics\n",
    "            metrics['cider'] = coco_eval.eval.get('CIDEr', 0.0)\n",
    "            metrics['spice'] = coco_eval.eval.get('SPICE', 0.0)\n",
    "\n",
    "            # Clean up temporary files\n",
    "            os.remove(gt_file)\n",
    "            os.remove(res_file)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error computing COCO metrics: {e}\")\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def evaluate_model(self, model, dataloader, max_samples=None):\n",
    "        \"\"\"\n",
    "        Comprehensive evaluation of the model\n",
    "        \"\"\"\n",
    "        print(\"Starting comprehensive evaluation...\")\n",
    "\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        references = []\n",
    "        predictions_dict = {}\n",
    "        references_dict = {}\n",
    "\n",
    "        sample_count = 0\n",
    "        max_samples = max_samples or len(dataloader.dataset)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(tqdm(dataloader, desc=\"Evaluating\")):\n",
    "                if sample_count >= max_samples:\n",
    "                    break\n",
    "\n",
    "                pixel_values = batch['pixel_values'].to(device)\n",
    "                image_ids = batch['image_ids']\n",
    "\n",
    "                # Generate captions\n",
    "                generated_captions = model.generate_caption(\n",
    "                    pixel_values,\n",
    "                    max_length=CONFIG['MAX_GENERATION_LENGTH'],\n",
    "                    do_sample=False\n",
    "                )\n",
    "\n",
    "                # Collect predictions and references\n",
    "                for i, (gen_cap, img_id) in enumerate(zip(generated_captions, image_ids)):\n",
    "                    # Get reference captions for this image\n",
    "                    ref_captions = dataloader.dataset.get_image_captions(img_id)\n",
    "\n",
    "                    predictions.append(gen_cap)\n",
    "                    references.append(ref_captions)\n",
    "\n",
    "                    predictions_dict[img_id] = gen_cap\n",
    "                    references_dict[img_id] = ref_captions\n",
    "\n",
    "                    sample_count += 1\n",
    "                    if sample_count >= max_samples:\n",
    "                        break\n",
    "\n",
    "        print(f\"Evaluated {sample_count} samples\")\n",
    "\n",
    "        # Compute all metrics\n",
    "        print(\"Computing BLEU scores...\")\n",
    "        bleu_scores = self.compute_bleu_scores(predictions, references)\n",
    "\n",
    "        print(\"Computing METEOR score...\")\n",
    "        meteor_score = self.compute_meteor_score(predictions, references)\n",
    "\n",
    "        print(\"Computing ROUGE-L score...\")\n",
    "        rouge_l_score = self.compute_rouge_l_score(predictions, references)\n",
    "\n",
    "        print(\"Computing CIDEr and SPICE scores...\")\n",
    "        coco_metrics = self.compute_coco_metrics(predictions_dict, references_dict)\n",
    "\n",
    "        # Combine all metrics\n",
    "        all_metrics = {\n",
    "            **bleu_scores,\n",
    "            'meteor': meteor_score,\n",
    "            'rouge_l': rouge_l_score,\n",
    "            **coco_metrics\n",
    "        }\n",
    "\n",
    "        return all_metrics, predictions[:10], references[:10]  # Return sample predictions\n",
    "\n",
    "# Initialize evaluation metrics\n",
    "evaluator = EvaluationMetrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T18:34:24.742515Z",
     "iopub.status.busy": "2025-07-28T18:34:24.742447Z",
     "iopub.status.idle": "2025-07-28T18:34:24.751380Z",
     "shell.execute_reply": "2025-07-28T18:34:24.751018Z",
     "shell.execute_reply.started": "2025-07-28T18:34:24.742508Z"
    },
    "id": "ofOQI7YSMWm9"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 8: Training Loop with Early Stopping and Curves\n",
    "# ============================================================================\n",
    "\n",
    "def train_model():\n",
    "    \"\"\"\n",
    "    Main training function with early stopping, loss curves, and accuracy curves\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"STARTING MODEL TRAINING\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Initialize model\n",
    "    model = ViTGPT2Model(\n",
    "        vit_model_name=CONFIG['VIT_MODEL'],\n",
    "        gpt2_model_name=CONFIG['GPT2_MODEL']\n",
    "    ).to(device)\n",
    "\n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "    # Initialize optimizer and scheduler\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=CONFIG['LEARNING_RATE'],\n",
    "        weight_decay=0.01,\n",
    "        betas=(0.9, 0.999)\n",
    "    )\n",
    "\n",
    "    num_training_steps = CONFIG['NUM_EPOCHS'] * len(train_dataloader)\n",
    "    num_warmup_steps = int(0.1 * num_training_steps)\n",
    "\n",
    "    scheduler = get_scheduler(\n",
    "        name=\"cosine\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "\n",
    "    # Training tracking variables\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    learning_rates = []\n",
    "\n",
    "    # Metric tracking for accuracy curves\n",
    "    bleu_1_scores = []\n",
    "    bleu_2_scores = []\n",
    "    bleu_3_scores = []\n",
    "    bleu_4_scores = []\n",
    "    meteor_scores = []\n",
    "    rouge_l_scores = []\n",
    "    cider_scores = []\n",
    "    spice_scores = []\n",
    "\n",
    "    # Early stopping variables\n",
    "    best_val_loss = float('inf')\n",
    "    best_bleu4_score = 0.0\n",
    "    patience_counter = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    print(f\"Training configuration:\")\n",
    "    print(f\"  Epochs: {CONFIG['NUM_EPOCHS']}\")\n",
    "    print(f\"  Batch size: {CONFIG['BATCH_SIZE']}\")\n",
    "    print(f\"  Learning rate: {CONFIG['LEARNING_RATE']}\")\n",
    "    print(f\"  Patience: {CONFIG['PATIENCE']}\")\n",
    "    print(f\"  Warmup steps: {num_warmup_steps}\")\n",
    "    print(f\"  Total training steps: {num_training_steps}\")\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(CONFIG['NUM_EPOCHS']):\n",
    "        print(f\"\\n{'='*20} EPOCH {epoch+1}/{CONFIG['NUM_EPOCHS']} {'='*20}\")\n",
    "\n",
    "        # =========================\n",
    "        # TRAINING PHASE\n",
    "        # =========================\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        num_train_batches = 0\n",
    "\n",
    "        train_pbar = tqdm(train_dataloader, desc=f\"Training Epoch {epoch+1}\")\n",
    "\n",
    "        for batch_idx, batch in enumerate(train_pbar):\n",
    "            try:\n",
    "                # Move data to device\n",
    "                pixel_values = batch['pixel_values'].to(device)\n",
    "                caption_tokens = batch['caption_tokens'].to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(pixel_values=pixel_values, caption_tokens=caption_tokens)\n",
    "                loss = outputs.loss\n",
    "\n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "\n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "                # Update weights\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Track metrics\n",
    "                total_train_loss += loss.item()\n",
    "                num_train_batches += 1\n",
    "\n",
    "                # Update progress bar\n",
    "                current_lr = scheduler.get_last_lr()[0] if hasattr(scheduler, 'get_last_lr') else CONFIG['LEARNING_RATE']\n",
    "                train_pbar.set_postfix({\n",
    "                    'loss': f\"{loss.item():.4f}\",\n",
    "                    'lr': f\"{current_lr:.2e}\"\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in training batch {batch_idx}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Calculate average training loss\n",
    "        avg_train_loss = total_train_loss / max(num_train_batches, 1)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        learning_rates.append(scheduler.get_last_lr()[0] if hasattr(scheduler, 'get_last_lr') else CONFIG['LEARNING_RATE'])\n",
    "\n",
    "        # =========================\n",
    "        # VALIDATION PHASE\n",
    "        # =========================\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        num_val_batches = 0\n",
    "\n",
    "        val_pbar = tqdm(val_dataloader, desc=f\"Validation Epoch {epoch+1}\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(val_pbar):\n",
    "                try:\n",
    "                    # Move data to device\n",
    "                    pixel_values = batch['pixel_values'].to(device)\n",
    "                    caption_tokens = batch['caption_tokens'].to(device)\n",
    "\n",
    "                    # Forward pass\n",
    "                    outputs = model(pixel_values=pixel_values, caption_tokens=caption_tokens)\n",
    "                    loss = outputs.loss\n",
    "\n",
    "                    total_val_loss += loss.item()\n",
    "                    num_val_batches += 1\n",
    "\n",
    "                    val_pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in validation batch {batch_idx}: {e}\")\n",
    "                    continue\n",
    "\n",
    "        # Calculate average validation loss\n",
    "        avg_val_loss = total_val_loss / max(num_val_batches, 1)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        # =========================\n",
    "        # EVALUATION METRICS\n",
    "        # =========================\n",
    "        print(f\"\\nComputing evaluation metrics for epoch {epoch+1}...\")\n",
    "\n",
    "        try:\n",
    "            # Evaluate on a subset of validation data for efficiency\n",
    "            metrics, sample_preds, sample_refs = evaluator.evaluate_model(\n",
    "                model, val_dataloader, max_samples=500\n",
    "            )\n",
    "\n",
    "            # Store metrics for curves\n",
    "            bleu_1_scores.append(metrics['bleu_1'])\n",
    "            bleu_2_scores.append(metrics['bleu_2'])\n",
    "            bleu_3_scores.append(metrics['bleu_3'])\n",
    "            bleu_4_scores.append(metrics['bleu_4'])\n",
    "            meteor_scores.append(metrics['meteor'])\n",
    "            rouge_l_scores.append(metrics['rouge_l'])\n",
    "            cider_scores.append(metrics['cider'])\n",
    "            spice_scores.append(metrics['spice'])\n",
    "\n",
    "            # Print metrics\n",
    "            print(f\"\\nEpoch {epoch+1} Results:\")\n",
    "            print(f\"  Training Loss: {avg_train_loss:.4f}\")\n",
    "            print(f\"  Validation Loss: {avg_val_loss:.4f}\")\n",
    "            print(f\"  BLEU-1: {metrics['bleu_1']:.4f}\")\n",
    "            print(f\"  BLEU-2: {metrics['bleu_2']:.4f}\")\n",
    "            print(f\"  BLEU-3: {metrics['bleu_3']:.4f}\")\n",
    "            print(f\"  BLEU-4: {metrics['bleu_4']:.4f}\")\n",
    "            print(f\"  METEOR: {metrics['meteor']:.4f}\")\n",
    "            print(f\"  ROUGE-L: {metrics['rouge_l']:.4f}\")\n",
    "            print(f\"  CIDEr: {metrics['cider']:.4f}\")\n",
    "            print(f\"  SPICE: {metrics['spice']:.4f}\")\n",
    "\n",
    "            # Print sample predictions\n",
    "            print(f\"\\nSample Predictions:\")\n",
    "            for i in range(min(3, len(sample_preds))):\n",
    "                print(f\"  Generated: {sample_preds[i]}\")\n",
    "                print(f\"  Reference: {sample_refs[i][0] if sample_refs[i] else 'N/A'}\")\n",
    "                print()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during evaluation: {e}\")\n",
    "            # Add default values if evaluation fails\n",
    "            bleu_1_scores.append(0.0)\n",
    "            bleu_2_scores.append(0.0)\n",
    "            bleu_3_scores.append(0.0)\n",
    "            bleu_4_scores.append(0.0)\n",
    "            meteor_scores.append(0.0)\n",
    "            rouge_l_scores.append(0.0)\n",
    "            cider_scores.append(0.0)\n",
    "            spice_scores.append(0.0)\n",
    "            metrics = {'bleu_4': 0.0}\n",
    "\n",
    "        # =========================\n",
    "        # EARLY STOPPING LOGIC\n",
    "        # =========================\n",
    "        current_bleu4 = metrics.get('bleu_4', 0.0)\n",
    "\n",
    "        # Save best model based on BLEU-4 score\n",
    "        if current_bleu4 > best_bleu4_score:\n",
    "            best_bleu4_score = current_bleu4\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_epoch = epoch + 1\n",
    "            patience_counter = 0\n",
    "\n",
    "            # Save best model\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_bleu4': best_bleu4_score,\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'train_losses': train_losses,\n",
    "                'val_losses': val_losses,\n",
    "                'metrics': metrics\n",
    "            }, CONFIG['MODEL_SAVE_PATH'])\n",
    "\n",
    "            print(f\"✓ Saved best model with BLEU-4: {best_bleu4_score:.4f}\")\n",
    "\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement for {patience_counter} epochs (best BLEU-4: {best_bleu4_score:.4f})\")\n",
    "\n",
    "            # Early stopping check\n",
    "            if patience_counter >= CONFIG['PATIENCE']:\n",
    "                print(f\"\\nEarly stopping triggered after {epoch + 1} epochs!\")\n",
    "                print(f\"Best epoch: {best_epoch} with BLEU-4: {best_bleu4_score:.4f}\")\n",
    "                break\n",
    "\n",
    "    print(f\"\\nTraining completed!\")\n",
    "    print(f\"Best model saved at epoch {best_epoch} with BLEU-4: {best_bleu4_score:.4f}\")\n",
    "\n",
    "    # Return training history\n",
    "    training_history = {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'learning_rates': learning_rates,\n",
    "        'bleu_1_scores': bleu_1_scores,\n",
    "        'bleu_2_scores': bleu_2_scores,\n",
    "        'bleu_3_scores': bleu_3_scores,\n",
    "        'bleu_4_scores': bleu_4_scores,\n",
    "        'meteor_scores': meteor_scores,\n",
    "        'rouge_l_scores': rouge_l_scores,\n",
    "        'cider_scores': cider_scores,\n",
    "        'spice_scores': spice_scores,\n",
    "        'best_epoch': best_epoch,\n",
    "        'best_bleu4': best_bleu4_score\n",
    "    }\n",
    "\n",
    "    return model, training_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T18:34:24.752374Z",
     "iopub.status.busy": "2025-07-28T18:34:24.752285Z",
     "iopub.status.idle": "2025-07-28T18:34:24.760362Z",
     "shell.execute_reply": "2025-07-28T18:34:24.760159Z",
     "shell.execute_reply.started": "2025-07-28T18:34:24.752367Z"
    },
    "id": "2j5iWUSrNaWn"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 9: Plotting Functions for Loss and Accuracy Curves\n",
    "# ============================================================================\n",
    "\n",
    "def plot_training_curves(training_history):\n",
    "    \"\"\"\n",
    "    Plot comprehensive training curves\n",
    "    This implements the \"loss curve\" and \"accuracy curve\" requirements\n",
    "    \"\"\"\n",
    "    print(\"Plotting training curves...\")\n",
    "\n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(20, 15))\n",
    "    fig.suptitle('Training Progress and Evaluation Metrics', fontsize=16, fontweight='bold')\n",
    "\n",
    "    epochs = range(1, len(training_history['train_losses']) + 1)\n",
    "\n",
    "    # 1. Loss Curves\n",
    "    axes[0, 0].plot(epochs, training_history['train_losses'], 'b-', label='Training Loss', linewidth=2)\n",
    "    axes[0, 0].plot(epochs, training_history['val_losses'], 'r-', label='Validation Loss', linewidth=2)\n",
    "    axes[0, 0].set_title('Training and Validation Loss', fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. Learning Rate\n",
    "    axes[0, 1].plot(epochs, training_history['learning_rates'], 'g-', linewidth=2)\n",
    "    axes[0, 1].set_title('Learning Rate Schedule', fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Learning Rate')\n",
    "    axes[0, 1].set_yscale('log')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. BLEU Scores\n",
    "    axes[0, 2].plot(epochs, training_history['bleu_1_scores'], 'purple', label='BLEU-1', linewidth=2)\n",
    "    axes[0, 2].plot(epochs, training_history['bleu_2_scores'], 'orange', label='BLEU-2', linewidth=2)\n",
    "    axes[0, 2].plot(epochs, training_history['bleu_3_scores'], 'brown', label='BLEU-3', linewidth=2)\n",
    "    axes[0, 2].plot(epochs, training_history['bleu_4_scores'], 'red', label='BLEU-4', linewidth=2)\n",
    "    axes[0, 2].set_title('BLEU Scores', fontweight='bold')\n",
    "    axes[0, 2].set_xlabel('Epoch')\n",
    "    axes[0, 2].set_ylabel('BLEU Score')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "    # 4. METEOR Score\n",
    "    axes[1, 0].plot(epochs, training_history['meteor_scores'], 'cyan', linewidth=2)\n",
    "    axes[1, 0].set_title('METEOR Score', fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('METEOR Score')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # 5. ROUGE-L Score\n",
    "    axes[1, 1].plot(epochs, training_history['rouge_l_scores'], 'magenta', linewidth=2)\n",
    "    axes[1, 1].set_title('ROUGE-L Score', fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('ROUGE-L Score')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # 6. CIDEr Score\n",
    "    axes[1, 2].plot(epochs, training_history['cider_scores'], 'navy', linewidth=2)\n",
    "    axes[1, 2].set_title('CIDEr Score', fontweight='bold')\n",
    "    axes[1, 2].set_xlabel('Epoch')\n",
    "    axes[1, 2].set_ylabel('CIDEr Score')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "    # 7. SPICE Score\n",
    "    axes[2, 0].plot(epochs, training_history['spice_scores'], 'darkgreen', linewidth=2)\n",
    "    axes[2, 0].set_title('SPICE Score', fontweight='bold')\n",
    "    axes[2, 0].set_xlabel('Epoch')\n",
    "    axes[2, 0].set_ylabel('SPICE Score')\n",
    "    axes[2, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # 8. Combined Metrics Overview\n",
    "    # Normalize metrics to 0-1 range for comparison\n",
    "    def normalize_scores(scores):\n",
    "        if not scores or max(scores) == 0:\n",
    "            return scores\n",
    "        return [s / max(scores) for s in scores]\n",
    "\n",
    "    norm_bleu4 = normalize_scores(training_history['bleu_4_scores'])\n",
    "    norm_meteor = normalize_scores(training_history['meteor_scores'])\n",
    "    norm_rouge = normalize_scores(training_history['rouge_l_scores'])\n",
    "    norm_cider = normalize_scores(training_history['cider_scores'])\n",
    "\n",
    "    axes[2, 1].plot(epochs, norm_bleu4, label='BLEU-4 (norm)', linewidth=2)\n",
    "    axes[2, 1].plot(epochs, norm_meteor, label='METEOR (norm)', linewidth=2)\n",
    "    axes[2, 1].plot(epochs, norm_rouge, label='ROUGE-L (norm)', linewidth=2)\n",
    "    axes[2, 1].plot(epochs, norm_cider, label='CIDEr (norm)', linewidth=2)\n",
    "    axes[2, 1].set_title('Normalized Metrics Comparison', fontweight='bold')\n",
    "    axes[2, 1].set_xlabel('Epoch')\n",
    "    axes[2, 1].set_ylabel('Normalized Score')\n",
    "    axes[2, 1].legend()\n",
    "    axes[2, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # 9. Training Summary\n",
    "    axes[2, 2].axis('off')\n",
    "    summary_text = f\"\"\"\n",
    "    Training Summary:\n",
    "\n",
    "    Best Epoch: {training_history['best_epoch']}\n",
    "    Best BLEU-4: {training_history['best_bleu4']:.4f}\n",
    "\n",
    "    Final Scores:\n",
    "    BLEU-1: {training_history['bleu_1_scores'][-1]:.4f}\n",
    "    BLEU-2: {training_history['bleu_2_scores'][-1]:.4f}\n",
    "    BLEU-3: {training_history['bleu_3_scores'][-1]:.4f}\n",
    "    BLEU-4: {training_history['bleu_4_scores'][-1]:.4f}\n",
    "    METEOR: {training_history['meteor_scores'][-1]:.4f}\n",
    "    ROUGE-L: {training_history['rouge_l_scores'][-1]:.4f}\n",
    "    CIDEr: {training_history['cider_scores'][-1]:.4f}\n",
    "    SPICE: {training_history['spice_scores'][-1]:.4f}\n",
    "\n",
    "    Total Epochs: {len(training_history['train_losses'])}\n",
    "    \"\"\"\n",
    "    axes[2, 2].text(0.1, 0.5, summary_text, fontsize=12, verticalalignment='center',\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightgray\", alpha=0.8))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/training_curves.png', dpi=300, bbox_inches='tight')\n",
    "    plt.savefig('plots/training_curves.pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"✓ Training curves saved to 'plots/training_curves.png' and 'plots/training_curves.pdf'\")\n",
    "\n",
    "def plot_loss_curves_only(training_history):\n",
    "    \"\"\"Plot detailed loss curves\"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    epochs = range(1, len(training_history['train_losses']) + 1)\n",
    "\n",
    "    # Loss curves\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, training_history['train_losses'], 'b-', label='Training Loss', linewidth=2, marker='o')\n",
    "    plt.plot(epochs, training_history['val_losses'], 'r-', label='Validation Loss', linewidth=2, marker='s')\n",
    "    plt.title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Loss difference\n",
    "    plt.subplot(1, 2, 2)\n",
    "    loss_diff = [v - t for t, v in zip(training_history['train_losses'], training_history['val_losses'])]\n",
    "    plt.plot(epochs, loss_diff, 'g-', label='Val - Train Loss', linewidth=2, marker='^')\n",
    "    plt.title('Overfitting Monitor', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss Difference')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/loss_curves_detailed.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-28T18:34:24.760783Z",
     "iopub.status.busy": "2025-07-28T18:34:24.760706Z",
     "iopub.status.idle": "2025-07-28T18:34:24.771830Z",
     "shell.execute_reply": "2025-07-28T18:34:24.771610Z",
     "shell.execute_reply.started": "2025-07-28T18:34:24.760777Z"
    },
    "id": "ZVZ5alPhNhtQ"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 10: Model Testing and Final Evaluation\n",
    "# ============================================================================\n",
    "\n",
    "def test_trained_model(model_path=None):\n",
    "    \"\"\"\n",
    "    Test the trained model on test set and generate comprehensive evaluation\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TESTING TRAINED MODEL\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Load the best model\n",
    "    model_path = model_path or CONFIG['MODEL_SAVE_PATH']\n",
    "\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Model file not found: {model_path}\")\n",
    "        return None\n",
    "\n",
    "    # Initialize model\n",
    "    model = ViTGPT2Model(\n",
    "        vit_model_name=CONFIG['VIT_MODEL'],\n",
    "        gpt2_model_name=CONFIG['GPT2_MODEL']\n",
    "    ).to(device)\n",
    "\n",
    "    # Load trained weights\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"✓ Loaded model from epoch {checkpoint['epoch']}\")\n",
    "    print(f\"✓ Best BLEU-4 from training: {checkpoint['best_bleu4']:.4f}\")\n",
    "\n",
    "    # Evaluate on test set\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_metrics, sample_preds, sample_refs = evaluator.evaluate_model(\n",
    "        model, test_dataloader, max_samples=1000\n",
    "    )\n",
    "\n",
    "    # Print comprehensive results\n",
    "    print(f\"\\n{'='*20} TEST SET RESULTS {'='*20}\")\n",
    "    print(f\"BLEU-1:   {test_metrics['bleu_1']:.4f}\")\n",
    "    print(f\"BLEU-2:   {test_metrics['bleu_2']:.4f}\")\n",
    "    print(f\"BLEU-3:   {test_metrics['bleu_3']:.4f}\")\n",
    "    print(f\"BLEU-4:   {test_metrics['bleu_4']:.4f}\")\n",
    "    print(f\"METEOR:   {test_metrics['meteor']:.4f}\")\n",
    "    print(f\"ROUGE-L:  {test_metrics['rouge_l']:.4f}\")\n",
    "    print(f\"CIDEr:    {test_metrics['cider']:.4f}\")\n",
    "    print(f\"SPICE:    {test_metrics['spice']:.4f}\")\n",
    "\n",
    "    # Generate sample predictions\n",
    "    print(f\"\\n{'='*20} SAMPLE PREDICTIONS {'='*20}\")\n",
    "    for i in range(min(10, len(sample_preds))):\n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        print(f\"  Generated: {sample_preds[i]}\")\n",
    "        print(f\"  Reference: {sample_refs[i][0] if sample_refs[i] else 'N/A'}\")\n",
    "        if len(sample_refs[i]) > 1:\n",
    "            for j, ref in enumerate(sample_refs[i][1:], 2):\n",
    "                print(f\"  Reference {j}: {ref}\")\n",
    "\n",
    "    # Save test results\n",
    "    test_results = {\n",
    "        'test_metrics': test_metrics,\n",
    "        'sample_predictions': sample_preds[:20],\n",
    "        'sample_references': sample_refs[:20],\n",
    "        'model_info': {\n",
    "            'model_path': model_path,\n",
    "            'epoch': checkpoint['epoch'],\n",
    "            'best_training_bleu4': checkpoint['best_bleu4']\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open('outputs/test_results.json', 'w') as f:\n",
    "        json.dump(test_results, f, indent=2)\n",
    "\n",
    "    print(f\"\\n✓ Test results saved to 'outputs/test_results.json'\")\n",
    "\n",
    "    return test_results\n",
    "\n",
    "def generate_sample_captions(model_path=None, num_samples=5):\n",
    "    \"\"\"Generate captions for sample images\"\"\"\n",
    "    print(\"Generating sample captions...\")\n",
    "\n",
    "    # Load model\n",
    "    model_path = model_path or CONFIG['MODEL_SAVE_PATH']\n",
    "    model = ViTGPT2Model().to(device)\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "\n",
    "    # Get sample images from test set\n",
    "    sample_images = test_dataset.data['images'][:num_samples]\n",
    "\n",
    "    plt.figure(figsize=(15, 3 * num_samples))\n",
    "\n",
    "    for i, img_info in enumerate(sample_images):\n",
    "        img_filename = img_info['file_name']\n",
    "        img_path = os.path.join(CONFIG['IMAGE_DIR'], img_filename)\n",
    "\n",
    "        if os.path.exists(img_path):\n",
    "            # Load and display image\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            plt.subplot(num_samples, 2, 2*i + 1)\n",
    "            plt.imshow(image)\n",
    "            plt.title(f\"Image: {img_filename}\")\n",
    "            plt.axis('off')\n",
    "\n",
    "            # Generate caption\n",
    "            pixel_values = image_processor(images=image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "            generated_caption = model.generate_caption(pixel_values, max_length=50)[0]\n",
    "\n",
    "            # Get reference captions\n",
    "            img_id = img_info['id']\n",
    "            ref_captions = test_dataset.get_image_captions(img_id)\n",
    "\n",
    "            # Display captions\n",
    "            plt.subplot(num_samples, 2, 2*i + 2)\n",
    "            plt.axis('off')\n",
    "            caption_text = f\"Generated Caption:\\n{generated_caption}\\n\\n\"\n",
    "            caption_text += \"Reference Captions:\\n\"\n",
    "            for j, ref in enumerate(ref_captions[:3], 1):\n",
    "                caption_text += f\"{j}. {ref}\\n\"\n",
    "\n",
    "            plt.text(0.05, 0.95, caption_text, transform=plt.gca().transAxes,\n",
    "                    fontsize=10, verticalalignment='top',\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightblue\", alpha=0.8))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('outputs/sample_captions.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"✓ Sample captions saved to 'outputs/sample_captions.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-07-28T19:08:22.327Z",
     "iopub.execute_input": "2025-07-28T18:34:24.772108Z",
     "iopub.status.busy": "2025-07-28T18:34:24.772038Z"
    },
    "id": "XazfLcLxNoKQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "VISION TRANSFORMER + GPT-2 IMAGE CAPTIONING\n",
      "================================================================================\n",
      "Starting training...\n",
      "============================================================\n",
      "STARTING MODEL TRAINING\n",
      "============================================================\n",
      "Initializing ViT-GPT2 Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ViT hidden size: 768\n",
      "✓ GPT-2 hidden size: 768\n",
      "✓ GPT-2 max position embeddings: 1024\n",
      "Total parameters: 211,419,648\n",
      "Trainable parameters: 211,419,648\n",
      "Training configuration:\n",
      "  Epochs: 20\n",
      "  Batch size: 16\n",
      "  Learning rate: 0.0001\n",
      "  Patience: 5\n",
      "  Warmup steps: 5460\n",
      "  Total training steps: 54600\n",
      "\n",
      "==================== EPOCH 1/20 ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11472dc734264533ae3b8e026f226780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 1:   0%|          | 0/2730 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ecf307ea554018ba258e120069f7af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch 1:   0%|          | 0/342 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing evaluation metrics for epoch 1...\n",
      "Starting comprehensive evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a097e2f77b0b461891a9ec6fcd526f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/342 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated 500 samples\n",
      "Computing BLEU scores...\n",
      "Error computing BLEU scores: float division by zero\n",
      "Computing METEOR score...\n",
      "Computing ROUGE-L score...\n",
      "Computing CIDEr and SPICE scores...\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 0, 'reflen': 694, 'guess': [0, 0, 0, 0], 'correct': [0, 0, 0, 0]}\n",
      "ratio: 1.440922190199653e-18\n",
      "Bleu_1: 0.000\n",
      "Bleu_2: 0.000\n",
      "Bleu_3: 0.000\n",
      "Bleu_4: 0.000\n",
      "computing METEOR score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 5280 tokens at 237197.32 tokens per second.\n",
      "PTBTokenizer tokenized 99 tokens at 6656.70 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METEOR: 0.000\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.000\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.000\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.1 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.2 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.2 sec].\n",
      "Threads( StanfordCoreNLP ) [1.429 seconds]\n",
      "Parsing test captions\n",
      "Threads( StanfordCoreNLP ) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 3.692 s\n",
      "SPICE: 0.000\n",
      "\n",
      "Epoch 1 Results:\n",
      "  Training Loss: 0.4241\n",
      "  Validation Loss: 0.2004\n",
      "  BLEU-1: 0.0000\n",
      "  BLEU-2: 0.0000\n",
      "  BLEU-3: 0.0000\n",
      "  BLEU-4: 0.0000\n",
      "  METEOR: 0.0000\n",
      "  ROUGE-L: 0.0000\n",
      "  CIDEr: 0.0000\n",
      "  SPICE: 0.0000\n",
      "\n",
      "Sample Predictions:\n",
      "  Generated: \n",
      "  Reference: the tarmac and airport runways divide the field into several orderly arranged rounded rectangles  next to which is buildings and a road .\n",
      "\n",
      "  Generated: \n",
      "  Reference: the tarmac and airport runways divide the field into several orderly arranged rounded rectangles  next to which is buildings and a road .\n",
      "\n",
      "  Generated: \n",
      "  Reference: the tarmac and airport runways divide the field into several orderly arranged rounded rectangles  next to which is buildings and a road .\n",
      "\n",
      "No improvement for 1 epochs (best BLEU-4: 0.0000)\n",
      "\n",
      "==================== EPOCH 2/20 ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e82bf1200924721bd0a36a8ca020f46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 2:   0%|          | 0/2730 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cda6a42633d43d8a03bc7b92e2cd7a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch 2:   0%|          | 0/342 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing evaluation metrics for epoch 2...\n",
      "Starting comprehensive evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94df9c0b0d0e403a81e6630fd1647584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/342 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated 500 samples\n",
      "Computing BLEU scores...\n",
      "Error computing BLEU scores: float division by zero\n",
      "Computing METEOR score...\n",
      "Computing ROUGE-L score...\n",
      "Computing CIDEr and SPICE scores...\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 5280 tokens at 229100.47 tokens per second.\n",
      "PTBTokenizer tokenized 99 tokens at 6645.73 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 0, 'reflen': 694, 'guess': [0, 0, 0, 0], 'correct': [0, 0, 0, 0]}\n",
      "ratio: 1.440922190199653e-18\n",
      "Bleu_1: 0.000\n",
      "Bleu_2: 0.000\n",
      "Bleu_3: 0.000\n",
      "Bleu_4: 0.000\n",
      "computing METEOR score...\n",
      "METEOR: 0.000\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.000\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.000\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 306.5 ms\n",
      "SPICE: 0.000\n",
      "\n",
      "Epoch 2 Results:\n",
      "  Training Loss: 0.1417\n",
      "  Validation Loss: 0.2036\n",
      "  BLEU-1: 0.0000\n",
      "  BLEU-2: 0.0000\n",
      "  BLEU-3: 0.0000\n",
      "  BLEU-4: 0.0000\n",
      "  METEOR: 0.0000\n",
      "  ROUGE-L: 0.0000\n",
      "  CIDEr: 0.0000\n",
      "  SPICE: 0.0000\n",
      "\n",
      "Sample Predictions:\n",
      "  Generated: \n",
      "  Reference: the tarmac and airport runways divide the field into several orderly arranged rounded rectangles  next to which is buildings and a road .\n",
      "\n",
      "  Generated: \n",
      "  Reference: the tarmac and airport runways divide the field into several orderly arranged rounded rectangles  next to which is buildings and a road .\n",
      "\n",
      "  Generated: \n",
      "  Reference: the tarmac and airport runways divide the field into several orderly arranged rounded rectangles  next to which is buildings and a road .\n",
      "\n",
      "No improvement for 2 epochs (best BLEU-4: 0.0000)\n",
      "\n",
      "==================== EPOCH 3/20 ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b6d7c292214448196537209145f6fec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 3:   0%|          | 0/2730 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac4649e3b9ab40edbf9cfc9a9fc5efe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Epoch 3:   0%|          | 0/342 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing evaluation metrics for epoch 3...\n",
      "Starting comprehensive evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8f99c2420d4419082bd935e508c3b9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/342 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated 500 samples\n",
      "Computing BLEU scores...\n",
      "Error computing BLEU scores: float division by zero\n",
      "Computing METEOR score...\n",
      "Computing ROUGE-L score...\n",
      "Computing CIDEr and SPICE scores...\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n",
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 0, 'reflen': 694, 'guess': [0, 0, 0, 0], 'correct': [0, 0, 0, 0]}\n",
      "ratio: 1.440922190199653e-18\n",
      "Bleu_1: 0.000\n",
      "Bleu_2: 0.000\n",
      "Bleu_3: 0.000\n",
      "Bleu_4: 0.000\n",
      "computing METEOR score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 5280 tokens at 265034.22 tokens per second.\n",
      "PTBTokenizer tokenized 99 tokens at 6711.75 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METEOR: 0.000\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.000\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.000\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 295.0 ms\n",
      "SPICE: 0.000\n",
      "\n",
      "Epoch 3 Results:\n",
      "  Training Loss: 0.1051\n",
      "  Validation Loss: 0.2296\n",
      "  BLEU-1: 0.0000\n",
      "  BLEU-2: 0.0000\n",
      "  BLEU-3: 0.0000\n",
      "  BLEU-4: 0.0000\n",
      "  METEOR: 0.0000\n",
      "  ROUGE-L: 0.0000\n",
      "  CIDEr: 0.0000\n",
      "  SPICE: 0.0000\n",
      "\n",
      "Sample Predictions:\n",
      "  Generated: \n",
      "  Reference: the tarmac and airport runways divide the field into several orderly arranged rounded rectangles  next to which is buildings and a road .\n",
      "\n",
      "  Generated: \n",
      "  Reference: the tarmac and airport runways divide the field into several orderly arranged rounded rectangles  next to which is buildings and a road .\n",
      "\n",
      "  Generated: \n",
      "  Reference: the tarmac and airport runways divide the field into several orderly arranged rounded rectangles  next to which is buildings and a road .\n",
      "\n",
      "No improvement for 3 epochs (best BLEU-4: 0.0000)\n",
      "\n",
      "==================== EPOCH 4/20 ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a064f65ac9ba4de6825f2d666f1405f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 4:   0%|          | 0/2730 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 11: Main Execution\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"VISION TRANSFORMER + GPT-2 IMAGE CAPTIONING\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Check if required files exist\n",
    "    required_files = [\"rsicd_train.json\", \"rsicd_val.json\", \"rsicd_test.json\"]\n",
    "    missing_files = [f for f in required_files if not os.path.exists(f)]\n",
    "\n",
    "    if missing_files:\n",
    "        print(f\"Missing required files: {missing_files}\")\n",
    "        print(\"Please run the dataset splitting cell first!\")\n",
    "        return\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Starting training...\")\n",
    "    trained_model, training_history = train_model()\n",
    "\n",
    "    # Plot training curves\n",
    "    print(\"\\nGenerating training curves...\")\n",
    "    plot_training_curves(training_history)\n",
    "    plot_loss_curves_only(training_history)\n",
    "\n",
    "    # Test the model\n",
    "    print(\"\\nTesting the trained model...\")\n",
    "    test_results = test_trained_model()\n",
    "\n",
    "    # Generate sample captions\n",
    "    print(\"\\nGenerating sample captions...\")\n",
    "    generate_sample_captions(num_samples=5)\n",
    "\n",
    "    # Save training history\n",
    "    with open('outputs/training_history.json', 'w') as f:\n",
    "        json.dump(training_history, f, indent=2)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TRAINING AND EVALUATION COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Files generated:\")\n",
    "    print(\"  - best_image_captioning_model.pth (best model)\")\n",
    "    print(\"  - plots/training_curves.png (comprehensive curves)\")\n",
    "    print(\"  - plots/loss_curves_detailed.png (detailed loss curves)\")\n",
    "    print(\"  - outputs/test_results.json (test evaluation)\")\n",
    "    print(\"  - outputs/training_history.json (training metrics)\")\n",
    "    print(\"  - outputs/sample_captions.png (sample predictions)\")\n",
    "\n",
    "    return trained_model, training_history, test_results\n",
    "\n",
    "# Run the complete pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    trained_model, training_history, test_results = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-07-28T19:08:22.327Z"
    },
    "id": "AZc_f6fWNt_2"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 12: Additional Utilities and Analysis\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_training_results(training_history):\n",
    "    \"\"\"Analyze training results and provide insights\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TRAINING ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Training stability analysis\n",
    "    train_losses = training_history['train_losses']\n",
    "    val_losses = training_history['val_losses']\n",
    "\n",
    "    # Calculate improvement rates\n",
    "    if len(train_losses) > 1:\n",
    "        train_improvement = (train_losses[0] - train_losses[-1]) / train_losses[0] * 100\n",
    "        val_improvement = (val_losses[0] - val_losses[-1]) / val_losses[0] * 100\n",
    "\n",
    "        print(f\"Loss Improvements:\")\n",
    "        print(f\"  Training loss improved by: {train_improvement:.2f}%\")\n",
    "        print(f\"  Validation loss improved by: {val_improvement:.2f}%\")\n",
    "\n",
    "    # Overfitting analysis\n",
    "    final_gap = val_losses[-1] - train_losses[-1]\n",
    "    avg_gap = np.mean([v - t for v, t in zip(val_losses, train_losses)])\n",
    "\n",
    "    print(f\"\\nOverfitting Analysis:\")\n",
    "    print(f\"  Final validation-training gap: {final_gap:.4f}\")\n",
    "    print(f\"  Average validation-training gap: {avg_gap:.4f}\")\n",
    "\n",
    "    if final_gap > 0.5:\n",
    "        print(\"  ⚠️  High overfitting detected\")\n",
    "    elif final_gap > 0.2:\n",
    "        print(\"  ⚠️  Moderate overfitting detected\")\n",
    "    else:\n",
    "        print(\"  ✓ Low overfitting - good generalization\")\n",
    "\n",
    "    # Best metrics summary\n",
    "    best_bleu4_idx = np.argmax(training_history['bleu_4_scores'])\n",
    "    best_meteor_idx = np.argmax(training_history['meteor_scores'])\n",
    "    best_cider_idx = np.argmax(training_history['cider_scores'])\n",
    "\n",
    "    print(f\"\\nBest Metric Epochs:\")\n",
    "    print(f\"  Best BLEU-4: {training_history['bleu_4_scores'][best_bleu4_idx]:.4f} at epoch {best_bleu4_idx + 1}\")\n",
    "    print(f\"  Best METEOR: {training_history['meteor_scores'][best_meteor_idx]:.4f} at epoch {best_meteor_idx + 1}\")\n",
    "    print(f\"  Best CIDEr: {training_history['cider_scores'][best_cider_idx]:.4f} at epoch {best_cider_idx + 1}\")\n",
    "\n",
    "def compare_with_baselines():\n",
    "    \"\"\"Compare results with typical baselines\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"BASELINE COMPARISON\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Typical baseline scores for image captioning\n",
    "    baselines = {\n",
    "        'Random': {'bleu_4': 0.01, 'meteor': 0.05, 'cider': 0.10},\n",
    "        'Template-based': {'bleu_4': 0.10, 'meteor': 0.15, 'cider': 0.30},\n",
    "        'CNN-RNN (basic)': {'bleu_4': 0.18, 'meteor': 0.22, 'cider': 0.60},\n",
    "        'CNN-LSTM (advanced)': {'bleu_4': 0.25, 'meteor': 0.27, 'cider': 0.85},\n",
    "        'Transformer (basic)': {'bleu_4': 0.28, 'meteor': 0.29, 'cider': 0.95},\n",
    "        'State-of-the-art': {'bleu_4': 0.35, 'meteor': 0.32, 'cider': 1.20}\n",
    "    }\n",
    "\n",
    "    print(\"Baseline Comparison (typical scores):\")\n",
    "    print(f\"{'Method':<20} {'BLEU-4':<10} {'METEOR':<10} {'CIDEr':<10}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for method, scores in baselines.items():\n",
    "        print(f\"{method:<20} {scores['bleu_4']:<10.3f} {scores['meteor']:<10.3f} {scores['cider']:<10.3f}\")\n",
    "\n",
    "    print(\"\\nNote: These are typical ranges. Actual performance depends on dataset and implementation.\")\n",
    "\n",
    "def save_model_summary():\n",
    "    \"\"\"Save a comprehensive model summary\"\"\"\n",
    "    summary = {\n",
    "        'model_architecture': 'ViT + GPT-2',\n",
    "        'vit_model': CONFIG['VIT_MODEL'],\n",
    "        'gpt2_model': CONFIG['GPT2_MODEL'],\n",
    "        'training_config': CONFIG,\n",
    "        'dataset_info': {\n",
    "            'train_samples': len(train_dataset),\n",
    "            'val_samples': len(val_dataset),\n",
    "            'test_samples': len(test_dataset)\n",
    "        },\n",
    "        'implementation_features': [\n",
    "            'Early stopping based on BLEU-4 score',\n",
    "            'Learning rate scheduling with warmup',\n",
    "            'Gradient clipping for stability',\n",
    "            'Comprehensive evaluation metrics',\n",
    "            'Image feature pooling to handle position embeddings',\n",
    "            'Loss and accuracy curve tracking'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    with open('outputs/model_summary.json', 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "\n",
    "    print(\"✓ Model summary saved to 'outputs/model_summary.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-07-28T19:08:22.327Z"
    },
    "id": "QCp3cZxGOS57"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 13: Final Execution and Summary\n",
    "# ============================================================================\n",
    "\n",
    "# Execute analysis functions if training history is available\n",
    "def final_analysis():\n",
    "    \"\"\"Run final analysis and cleanup\"\"\"\n",
    "    try:\n",
    "        if 'training_history' in globals():\n",
    "            analyze_training_results(training_history)\n",
    "            compare_with_baselines()\n",
    "            save_model_summary()\n",
    "        else:\n",
    "            print(\"Training history not available. Run training first.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in final analysis: {e}\")\n",
    "\n",
    "# Print execution summary\n",
    "def print_execution_summary():\n",
    "    \"\"\"Print a summary of what has been implemented\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"IMPLEMENTATION SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    implemented_features = [\n",
    "        \"✓ Create JSON files when split - Dataset splitting into train/val/test with COCO format\",\n",
    "        \"✓ Model (ViT + GPT2) - Vision Transformer encoder with GPT-2 decoder\",\n",
    "        \"✓ Early stopping - Based on BLEU-4 score with configurable patience\",\n",
    "        \"✓ Loss curve - Training and validation loss tracking and plotting\",\n",
    "        \"✓ Accuracy curve - Multiple evaluation metrics tracked over epochs\",\n",
    "        \"✓ Evaluate BLEU 1-4 - All BLEU variants computed using Hugging Face evaluate\",\n",
    "        \"✓ Evaluate METEOR - METEOR metric using Hugging Face evaluate\",\n",
    "        \"✓ Evaluate CIDEr - CIDEr metric using COCO evaluation tools\",\n",
    "        \"✓ Evaluate ROUGE_L - ROUGE-L metric using Hugging Face evaluate\",\n",
    "        \"✓ Evaluate SPICE - SPICE metric using COCO evaluation tools\"\n",
    "    ]\n",
    "\n",
    "    for feature in implemented_features:\n",
    "        print(feature)\n",
    "\n",
    "    print(\"\\nAdditional Features Implemented:\")\n",
    "    additional_features = [\n",
    "        \"• Comprehensive dataset class with proper COCO format handling\",\n",
    "        \"• Position embedding fix using adaptive pooling\",\n",
    "        \"• Gradient clipping and learning rate scheduling\",\n",
    "        \"• Detailed training progress visualization\",\n",
    "        \"• Sample caption generation with visual examples\",\n",
    "        \"• Model checkpointing and best model saving\",\n",
    "        \"• Overfitting analysis and training stability metrics\",\n",
    "        \"• Error handling and robust training pipeline\",\n",
    "        \"• Configurable hyperparameters\",\n",
    "        \"• Comprehensive logging and result saving\"\n",
    "    ]\n",
    "\n",
    "    for feature in additional_features:\n",
    "        print(feature)\n",
    "\n",
    "    print(\"\\nGenerated Files:\")\n",
    "    output_files = [\n",
    "        \"• rsicd_train.json, rsicd_val.json, rsicd_test.json - Dataset splits\",\n",
    "        \"• best_image_captioning_model.pth - Best trained model\",\n",
    "        \"• plots/training_curves.png - Comprehensive training visualization\",\n",
    "        \"• plots/loss_curves_detailed.png - Detailed loss analysis\",\n",
    "        \"• outputs/test_results.json - Final evaluation results\",\n",
    "        \"• outputs/training_history.json - Complete training metrics\",\n",
    "        \"• outputs/sample_captions.png - Visual caption examples\",\n",
    "        \"• outputs/model_summary.json - Model and training summary\"\n",
    "    ]\n",
    "\n",
    "    for file_info in output_files:\n",
    "        print(file_info)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ALL REQUIREMENTS SUCCESSFULLY IMPLEMENTED!\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "# Run final analysis\n",
    "final_analysis()\n",
    "\n",
    "# Print execution summary\n",
    "print_execution_summary()\n",
    "\n",
    "# ============================================================================\n",
    "# END OF COMPLETE IMPLEMENTATION\n",
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0ea4402503be43c4af682772898dfea7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_18ad727136da4b31814855ce1eca35b5",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3bad1a902ba249b9854979d85a0af0f6",
      "value": 0
     }
    },
    "18ad727136da4b31814855ce1eca35b5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2a78bd1e63fd40328b9fc7849f968ae0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d6c27fe6c52b402c8b3b7a4df3736559",
       "IPY_MODEL_0ea4402503be43c4af682772898dfea7",
       "IPY_MODEL_956003538f464073a066b70b9440dd82"
      ],
      "layout": "IPY_MODEL_8681c344d6db448c8e4d193b6428a1aa"
     }
    },
    "333243f6bf76493fbf3df8dd3082b3da": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3bad1a902ba249b9854979d85a0af0f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "818cb93aaf2c4fad922d08b87e2a46cd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8681c344d6db448c8e4d193b6428a1aa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "956003538f464073a066b70b9440dd82": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_818cb93aaf2c4fad922d08b87e2a46cd",
      "placeholder": "​",
      "style": "IPY_MODEL_333243f6bf76493fbf3df8dd3082b3da",
      "value": " 0/1 [00:03&lt;?, ?it/s]"
     }
    },
    "b61b39a4f17f46c7861d337a85a04a9e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cfc381380edf4b4ea2bc5fcc017841a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d6c27fe6c52b402c8b3b7a4df3736559": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b61b39a4f17f46c7861d337a85a04a9e",
      "placeholder": "​",
      "style": "IPY_MODEL_cfc381380edf4b4ea2bc5fcc017841a4",
      "value": "Epoch 1 Training:   0%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
